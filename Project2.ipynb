{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Packages and dataset load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "# import dataframe_image as df___i\n",
    "\n",
    "color = {\"granate\":\"#BA4A00\",\n",
    "         \"amarillo\":\"#F5B041\",\n",
    "         \"verde\":\"#148F77\",\n",
    "         \"blue\":\"#0051A2\",\n",
    "         \"red\": \"#DD1717\"}\n",
    "color_palette = [color[\"blue\"], 'darkorchid', color['verde'], color['amarillo'],'gray', 'cornflowerblue', color['red']]\n",
    "sb.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Dry_Bean_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Auxiliar functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(models_names: list, columns_names: list, k1: int):\n",
    "    header = pd.MultiIndex.from_product([models_names, columns_names])\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    df['KFold'] = np.arange(1, k1+1)\n",
    "    df.set_index('KFold', inplace=True)\n",
    "    return df\n",
    "\n",
    "def twolevelcv(X: np.array, y: np.array, k1: int, k2: int, models: list, params: dict, rs: int):\n",
    "    \"\"\"Allows to compute two level crossvalidation.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Features (numeric)\n",
    "        y (np.array): Class (objective variable)\n",
    "        k1 (int): Nº of outer folds\n",
    "        k2 (int): Nº of inner folds\n",
    "        models (list): List of models for comparison\n",
    "        params (dict): Dictionary including the set of parameters. In this case we only tune 1 parameter per model.\n",
    "        rs (int): Random state\n",
    "    Returns:\n",
    "        df: Dataframe\n",
    "    \"\"\"\n",
    "    test_error_dict = {}\n",
    "    k = 0\n",
    "    names = [type(m).__name__ for m in models]\n",
    "    col_names = ['Param. Value', 'Error']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = StratifiedKFold(k1, shuffle = True, random_state=rs)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k += 1\n",
    "        kf2 = StratifiedKFold(k2, shuffle = True, random_state=rs)\n",
    "        print(f'Computing KFold {k}/{k1}...')\n",
    "        # second level split\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            X_train = X[train_idx2, :]\n",
    "            y_train = y[train_idx2]\n",
    "            X_test = X[test_idx2, :]\n",
    "            y_test = y[test_idx2]\n",
    "            for name, model in zip(names, models):\n",
    "                if name != 'DummyClassifier':\n",
    "                    pname = list(params[name].keys())[0]\n",
    "                    error_test = []\n",
    "                    for p_ in params[name][pname]:\n",
    "                        pdict = {pname: p_}\n",
    "                        model = model.set_params(**pdict)\n",
    "                        # train the model\n",
    "                        model.fit(X_train, y_train)\n",
    "                        # evaluate performance\n",
    "                        pred2_test = model.predict(X_test)\n",
    "                        error_test.append(np.sum(pred2_test != y_test)/ y_test.shape[0])\n",
    "                    min_param = params[name][pname][np.argmin(error_test)]\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    pred2_test = model.predict(X_test)\n",
    "                    error_test = np.sum(pred2_test != y_test)/ y_test.shape[0]\n",
    "                    min_param = np.NaN\n",
    "                df.loc(axis = 1)[name, 'Error'][k] = np.min(error_test)\n",
    "                df.loc(axis = 1)[name, 'Param. Value'][k] = min_param\n",
    "    return df, test_idx1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad43814",
   "metadata": {},
   "source": [
    "# **1 - Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· NUMBER OF FEATURES: 16\n",
      "\n",
      "· FEATURES:\n",
      " ['Area' 'Perimeter' 'MajorAxisLength' 'MinorAxisLength' 'AspectRation'\n",
      " 'Eccentricity' 'ConvexArea' 'EquivDiameter' 'Extent' 'Solidity'\n",
      " 'roundness' 'Compactness' 'ShapeFactor1' 'ShapeFactor2' 'ShapeFactor3'\n",
      " 'ShapeFactor4']\n",
      "\n",
      "· NUMBER OF DATA POINTS: 13611\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns.values\n",
    "X = df.drop(columns='Class').values\n",
    "y = df['roundness']\n",
    "\n",
    "\n",
    "print('· NUMBER OF FEATURES:', X.shape[1])\n",
    "print('\\n· FEATURES:\\n', columns[:-1])\n",
    "print('\\n· NUMBER OF DATA POINTS:', X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part A. *Linear regression.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part B. *Other models. Evaluation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "(13611, 15)\n",
      "169.77568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     53\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 54\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_2/lib/python3.8/site-packages/torch/optim/optimizer.py:112\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_2/lib/python3.8/site-packages/torch/autograd/profiler.py:446\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs)\n\u001b[1;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib.pylab import figure, plot, xlabel, ylabel, legend, ylim, show\n",
    "import sklearn.linear_model as lm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "x = df.drop(columns='Class').drop(columns='MajorAxisLength').values\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "y = df['roundness']\n",
    "y = np.array(y)\n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, num_input, num_output, num_hidden):\n",
    "        super(ANN, self).__init__()\n",
    "        self.Net = nn.Sequential(\n",
    "            nn.Linear(num_input, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_output),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.Net(input)\n",
    "\n",
    "model = ANN(x.shape[1], y.shape[1], 32).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "x_train = torch.from_numpy(x).to(device)\n",
    "y_train = torch.from_numpy(y).to(device)\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_set = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 1000\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_ = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    for x, y in train_set:\n",
    "        output = model(x)\n",
    "        loss = loss_func(output, y)\n",
    "        loss_.append(loss.cpu().detach().numpy())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if i % 100 == 0:\n",
    "        print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84749925] tensor([0.7396])\n"
     ]
    }
   ],
   "source": [
    "output = model(x_train)\n",
    "print(output.cpu().detach().numpy()[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad43814",
   "metadata": {},
   "source": [
    "# **2 - Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying object without editing the original\n",
    "df_ = df.copy(deep=True)\n",
    "# Doing this we can choose to use outliers filter or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· NUMBER OF FEATURES: 16\n",
      "\n",
      "· FEATURES: ['Area' 'Perimeter' 'MajorAxisLength' 'MinorAxisLength' 'AspectRation'\n",
      " 'Eccentricity' 'ConvexArea' 'EquivDiameter' 'Extent' 'Solidity'\n",
      " 'roundness' 'Compactness' 'ShapeFactor1' 'ShapeFactor2' 'ShapeFactor3'\n",
      " 'ShapeFactor4']\n",
      "\n",
      "· NUMBER OF DATA POINTS: 13611\n",
      "\n",
      "· CLASSES: ['SEKER' 'BARBUNYA' 'BOMBAY' 'CALI' 'HOROZ' 'SIRA' 'DERMASON']\n",
      "\n",
      "· NUMBER OF CLASSES: 7\n"
     ]
    }
   ],
   "source": [
    "columns = df_.columns.values\n",
    "X = df_.drop(columns='Class').values\n",
    "y = df_['Class']\n",
    "le = LabelEncoder()\n",
    "y_ = le.fit_transform(y)\n",
    "classes = y.unique()\n",
    "\n",
    "print('· NUMBER OF FEATURES:', X.shape[1])\n",
    "print('\\n· FEATURES:', columns[:-1])\n",
    "print('\\n· NUMBER OF DATA POINTS:', X.shape[0])\n",
    "print('\\n· CLASSES:', classes)\n",
    "print('\\n· NUMBER OF CLASSES:', len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered outliers: 298\n"
     ]
    }
   ],
   "source": [
    "Threshold_ = 3\n",
    "outlier_index = []\n",
    "df_ = pd.DataFrame(columns=df.columns)\n",
    "index = 0\n",
    "for K in classes:\n",
    "    outlier_index = []\n",
    "    a = df.loc[df[\"Class\"] == K]\n",
    "    value = a.drop(columns='Class').values\n",
    "    for j in range(16):\n",
    "        std = np.std(value[:, j])\n",
    "        mean = np.mean(value[:, j])\n",
    "        for i in range(value[:, j].shape[0]):\n",
    "            if (value[i, j] - mean) / std > Threshold_:\n",
    "                outlier_index.append(i + index)\n",
    "    index = i + index + 1\n",
    "    outlier_index = np.unique(outlier_index)\n",
    "    a = a.drop(outlier_index)\n",
    "    df_ = pd.concat([df_,a])\n",
    "df_.reset_index(drop=True, inplace=True)\n",
    "print(f'Filtered outliers: {df.shape[0] - df_.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarization of the dataset\n",
    "sc = StandardScaler()\n",
    "X_stdz = sc.fit_transform(X)\n",
    "df_stdz = pd.DataFrame(columns = columns[:-1], data = X_stdz)\n",
    "df_stdz['Class'] = y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Logistic regression *vs.* Neural Network *vs.* Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # evaluate the model and collect the scores\n",
    "# n_scores = cross_val_score(model, X_stdz, y_, scoring = 'accuracy', cv=cv, n_jobs=-1)\n",
    "# # report the model performance\n",
    "# print('Mean Accuracy: %.3f (+-%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Cross-Validation table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KFold 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:28<01:59, 39.67s/it]"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "lam = np.logspace(-6, 2, 100)\n",
    "C = 1/ lam\n",
    "# C = [200000000, 10000000, 0.1519911082952933, 0.2848035868435805 ]\n",
    "params['LogisticRegression'] = {'C': C}\n",
    "params['DummyClassifier'] = [None]\n",
    "params['MLPClassifier'] = {'hidden_layer_sizes': [(8, ), (16, ), (20, )]}\n",
    "models = [LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000000, random_state= random_state, tol = 0.003, n_jobs = -1),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=random_state),\n",
    "        MLPClassifier(solver='adam', activation='logistic', alpha=1e-4, random_state=random_state, max_iter=1000, \n",
    "        early_stopping=True, validation_fraction=0.2, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.01)]\n",
    "k1 = 10\n",
    "k2 = 10\n",
    "Table, test_set_outer = twolevelcv(X = X_stdz, y = y_, k1 = k1, k2 = k2, models = models, params = params, rs = random_state)\n",
    "Table.to_csv('Results/Test2_saga.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">LogisticRegression</th>\n",
       "      <th colspan=\"2\" halign=\"left\">DummyClassifier</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLPClassifier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Param. Value</th>\n",
       "      <th>Error</th>\n",
       "      <th>Param. Value</th>\n",
       "      <th>Error</th>\n",
       "      <th>Param. Value</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KFold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000000</td>\n",
       "      <td>0.040033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788399</td>\n",
       "      <td>(16,)</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200000000</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78449</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.035102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200000000</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78449</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.034286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200000000</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78449</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.035918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200000000</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78449</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.035918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.151991</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.790204</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.151991</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.790204</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.151991</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.790204</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.041633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.151991</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.790204</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.042449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.151991</td>\n",
       "      <td>0.042449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788571</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>0.044082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LogisticRegression           DummyClassifier           MLPClassifier  \\\n",
       "            Param. Value     Error    Param. Value     Error  Param. Value   \n",
       "KFold                                                                        \n",
       "1              200000000  0.040033             NaN  0.788399         (16,)   \n",
       "2              200000000  0.034286             NaN   0.78449          (8,)   \n",
       "3              200000000  0.034286             NaN   0.78449          (8,)   \n",
       "4              200000000  0.034286             NaN   0.78449          (8,)   \n",
       "5              200000000  0.034286             NaN   0.78449          (8,)   \n",
       "6               0.151991  0.035918             NaN  0.790204          (8,)   \n",
       "7               0.151991  0.035918             NaN  0.790204          (8,)   \n",
       "8               0.151991  0.033469             NaN  0.790204          (8,)   \n",
       "9               0.151991  0.033469             NaN  0.790204          (8,)   \n",
       "10              0.151991  0.042449             NaN  0.788571          (8,)   \n",
       "\n",
       "                 \n",
       "          Error  \n",
       "KFold            \n",
       "1      0.039216  \n",
       "2      0.035102  \n",
       "3      0.034286  \n",
       "4      0.035918  \n",
       "5      0.035918  \n",
       "6      0.040816  \n",
       "7          0.04  \n",
       "8      0.041633  \n",
       "9      0.042449  \n",
       "10     0.044082  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table = Table.round(3)\n",
    "Table.to_csv(r'Results\\Table_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1361,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_outer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Stadistical Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def McNemar(models: list, X: np.array, y: np.array, k1: int, rs: int):\n",
    "    kf1 = StratifiedKFold(k1, shuffle = True, random_state=rs)\n",
    "    k = 0\n",
    "    # setting up all the possible combinations between the different models\n",
    "    matrix = dict.fromkeys(combinations(range(len(models)), 2))\n",
    "    for train_idx, test_idx in kf1.split(X, y):\n",
    "        test_size = test_idx.shape[0]\n",
    "        yABC = np.empty(shape=(len(models), test_size))\n",
    "        for i, model in enumerate(models):\n",
    "            model.fit(X[train_idx,:], y[train_idx])\n",
    "            y_pred = model.predict(X[test_idx, :])\n",
    "            yABC[i, :] = 1*(y_pred == y[test_idx])\n",
    "        for j in list(matrix.keys()):\n",
    "            if k == 0:\n",
    "                matrix[j] = np.empty(shape=(k1, 4))\n",
    "            n11 = np.sum(yABC[j[0],:]*yABC[j[1],:])\n",
    "            n12 = np.sum(yABC[j[0],:]*(1-yABC[j[1],:]))\n",
    "            n21 = np.sum(yABC[j[1],:]*(1-yABC[j[0],:]))\n",
    "            n22 = np.sum((1-yABC[0,:])*(1-yABC[1,:]))\n",
    "            matrix[j][k] = np.array([n11, n12, n21, n22])\n",
    "        k+=1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "models = [LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state= random_state, C = 0.07),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=random_state),\n",
    "        MLPClassifier(solver='adam', activation='logistic', alpha=1e-4, random_state=random_state, max_iter=1000, hidden_layer_sizes=(8, ),\n",
    "        early_stopping=True, validation_fraction=0.2, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.01)]\n",
    "k1 = 10\n",
    "m = McNemar(models, X_stdz[test_set_outer, :], y_[test_set_outer], k1, rs = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the matrix with shape (3: number of models, 10: nº of k-folds, 4: matrix shape)\n",
    "\n",
    "The matrix is squeezed so we have:\n",
    "$$\\begin{bmatrix}\n",
    " n_{11}    & n_{12}  \\\\\n",
    " n_{21}    & n_{22}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here is $[ n_{11}, n_{12}, n_{21}, n_{22} ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0:$ Model A has the same performarnce as model B \n",
    "\n",
    "$H_1:$ Model A and model B has different performance\n",
    "\n",
    "small p_value-> we discard H0 -> Model A and Model B have different performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we set:\n",
    "\n",
    "**Model 0**: LR\n",
    "\n",
    "**Model 1**: Baseline\n",
    "\n",
    "**Model 2**: MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***P-Values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = list(combinations(range(len(models)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "pv_dict = dict.fromkeys(combs)\n",
    "for j in combs:\n",
    "    vals = m[j][:, 1:3]\n",
    "    pv_dict[j] = [binom.cdf(min(vals[i]), n = sum(vals[i]), p = 1/2) for i in range(len(vals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McNemar_pv = pd.DataFrame(columns = combs, index = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for n in pv_dict.values():\n",
    "    print(f'{combs[i]}')\n",
    "    for j, k in enumerate(n):\n",
    "        McNemar_pv[combs[i]][j] = \"{:.3e}\".format(k)\n",
    "        print(\"{:.3e}\".format(k))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "McNemar_pv.to_csv('McNemar_pv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcs(mat: np.array):\n",
    "    \"\"\"Calculate f y g from a McNemar Matrix\n",
    "    Args:\n",
    "        matrix (np.array): McNemar matrix from one K-fold\n",
    "    Returns:\n",
    "        _type_: f and g\n",
    "    \"\"\"\n",
    "    n = mat.sum()\n",
    "    n12 = mat[1]\n",
    "    n21 = mat[2]\n",
    "    E_th = (n12 - n21)/n \n",
    "    Q = (n**2 * (n+1) * (E_th +1) * (1-E_th)) / (n * (n12 + n21) - (n12 - n21)**2)\n",
    "    f = (Q-1)*(E_th+1)/2\n",
    "    g = (Q-1)*(1-E_th)/2\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval(mat: np.array, alpha: float):\n",
    "    \"\"\"McNemar confidence interval\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The desired confidence (should be 0.05)  \n",
    "        f (_type_): output of calcs\n",
    "        g (_type_): output of calcs \n",
    "\n",
    "    Returns:\n",
    "        _type_: left and right bounds from the interval\n",
    "    \"\"\"\n",
    "    f,g = calcs(mat)\n",
    "    theta_L = 2*beta.ppf(alpha, f, g) - 1\n",
    "    theta_R = 2*beta.ppf(1 - alpha/2, f, g) - 1\n",
    "    return theta_L, theta_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "[0.58, 0.74]\n",
      "[0.61, 0.75]\n",
      "[0.58, 0.74]\n",
      "[0.6, 0.74]\n",
      "[0.64, 0.79]\n",
      "[0.54, 0.69]\n",
      "[0.57, 0.73]\n",
      "[0.61, 0.76]\n",
      "[0.57, 0.73]\n",
      "[0.58, 0.75]\n",
      "(0, 2)\n",
      "[-0.01, 0.06]\n",
      "[-0.0, 0.07]\n",
      "[-0.03, 0.02]\n",
      "[-0.02, 0.03]\n",
      "[0.01, 0.06]\n",
      "[-0.01, 0.06]\n",
      "[-0.02, 0.02]\n",
      "[-0.02, 0.02]\n",
      "[-0.06, 0.01]\n",
      "[-0.01, 0.03]\n",
      "(1, 2)\n",
      "[-0.71, -0.55]\n",
      "[-0.73, -0.57]\n",
      "[-0.73, -0.57]\n",
      "[-0.73, -0.58]\n",
      "[-0.77, -0.62]\n",
      "[-0.67, -0.51]\n",
      "[-0.72, -0.55]\n",
      "[-0.75, -0.59]\n",
      "[-0.73, -0.57]\n",
      "[-0.73, -0.56]\n"
     ]
    }
   ],
   "source": [
    "for i in m:\n",
    "    print(i)\n",
    "    for mat in m[i]:\n",
    "        theta_L, theta_R = interval(mat, 0.05)\n",
    "        print(f'[{np.round(theta_L, 2)}, {np.round(theta_R, 2)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Train logistic regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the fourth exercise do we have to repeat the parameter selection process or can just go ahead with the best parameter selection for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ca68e7b73839bd75b2d076de248efc7bf9c399b12d8560f2d058acabfd392b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
