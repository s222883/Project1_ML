{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Packages and dataset load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "# import dataframe_image as df___i\n",
    "\n",
    "color = {\"granate\":\"#BA4A00\",\n",
    "         \"amarillo\":\"#F5B041\",\n",
    "         \"verde\":\"#148F77\",\n",
    "         \"blue\":\"#0051A2\",\n",
    "         \"red\": \"#DD1717\"}\n",
    "color_palette = [color[\"blue\"], 'darkorchid', color['verde'], color['amarillo'],'gray', 'cornflowerblue', color['red']]\n",
    "sb.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Dry_Bean_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Auxiliar functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(models_names: list, columns_names: list, k1: int):\n",
    "    header = pd.MultiIndex.from_product([models_names, columns_names])\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    df['KFold'] = np.arange(1, k1+1)\n",
    "    df.set_index('KFold', inplace=True)\n",
    "    return df\n",
    "\n",
    "def twolevelcv(X: np.array, y: np.array, k1: int, k2: int, models: list, params: dict, rs: int):\n",
    "    \"\"\"Allows to compute two level crossvalidation.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Features (numeric)\n",
    "        y (np.array): Class (objective variable)\n",
    "        k1 (int): Nº of outer folds\n",
    "        k2 (int): Nº of inner folds\n",
    "        models (list): List of models for comparison\n",
    "        params (dict): Dictionary including the set of parameters. In this case we only tune 1 parameter per model.\n",
    "        rs (int): Random state\n",
    "    Returns:\n",
    "        df: Dataframe\n",
    "    \"\"\"\n",
    "    test_error_dict = {}\n",
    "    k = 0\n",
    "    names = [type(m).__name__ for m in models]\n",
    "    col_names = ['Param. Value', 'Error']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = StratifiedKFold(k1, shuffle = True, random_state=rs)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k += 1\n",
    "        kf2 = StratifiedKFold(k2, shuffle = True, random_state=rs)\n",
    "        print(f'Computing KFold {k}/{k1}...')\n",
    "        # second level split\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            X_train = X[train_idx2, :]\n",
    "            y_train = y[train_idx2]\n",
    "            X_test = X[test_idx2, :]\n",
    "            y_test = y[test_idx2]\n",
    "            for name, model in zip(names, models):\n",
    "                if name != 'DummyClassifier':\n",
    "                    pname = list(params[name].keys())[0]\n",
    "                    error_test = []\n",
    "                    for p_ in params[name][pname]:\n",
    "                        pdict = {pname: p_}\n",
    "                        model = model.set_params(**pdict)\n",
    "                        # train the model\n",
    "                        model.fit(X_train, y_train)\n",
    "                        # evaluate performance\n",
    "                        pred2_test = model.predict(X_test)\n",
    "                        error_test.append(np.sum(pred2_test != y_test)/ y_test.shape[0])\n",
    "                    min_param = params[name][pname][np.argmin(error_test)]\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    pred2_test = model.predict(X_test)\n",
    "                    error_test = np.sum(pred2_test != y_test)/ y_test.shape[0]\n",
    "                    min_param = np.NaN\n",
    "                df.loc(axis = 1)[name, 'Error'][k] = np.min(error_test)\n",
    "                df.loc(axis = 1)[name, 'Param. Value'][k] = min_param\n",
    "    return df, test_idx1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad43814",
   "metadata": {},
   "source": [
    "# **1 - Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· NUMBER OF FEATURES: 16\n",
      "\n",
      "· FEATURES:\n",
      " ['Area' 'Perimeter' 'MajorAxisLength' 'MinorAxisLength' 'AspectRation'\n",
      " 'Eccentricity' 'ConvexArea' 'EquivDiameter' 'Extent' 'Solidity'\n",
      " 'roundness' 'Compactness' 'ShapeFactor1' 'ShapeFactor2' 'ShapeFactor3'\n",
      " 'ShapeFactor4']\n",
      "\n",
      "· NUMBER OF DATA POINTS: 13611\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns.values\n",
    "X = df.drop(columns='Class').values\n",
    "y = df['roundness']\n",
    "\n",
    "\n",
    "print('· NUMBER OF FEATURES:', X.shape[1])\n",
    "print('\\n· FEATURES:\\n', columns[:-1])\n",
    "print('\\n· NUMBER OF DATA POINTS:', X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part A. *Linear regression.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part B. *Other models. Evaluation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training data x and label y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13611, 9)\n"
     ]
    }
   ],
   "source": [
    "label_feature = 'AspectRation'\n",
    "\n",
    "x = df.drop(columns=['Class', label_feature, 'MajorAxisLength', 'MinorAxisLength', \\\n",
    "                     'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', \\\n",
    "                    'ShapeFactor4']).values\n",
    "Class = df['Class']\n",
    "class_label = []\n",
    "dictionary = {\n",
    "    'SEKER': 0,\n",
    "    'BARBUNYA': 1,\n",
    "    'BOMBAY': 2, \n",
    "    'CALI': 3,\n",
    "    'HOROZ': 4,\n",
    "    'SIRA': 5,\n",
    "    'DERMASON': 6\n",
    "}\n",
    "for i in Class:\n",
    "    class_label.append(dictionary[i])\n",
    "\n",
    "class_label = np.array(class_label)\n",
    "x = np.array(x)\n",
    "x = np.insert(x, x.shape[1], class_label, axis=1)\n",
    "y = df[label_feature]\n",
    "y = np.array(y)\n",
    "y = y.reshape((-1,1))\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "print(x.shape)\n",
    "# X = x\n",
    "N = x.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standard = x - np.ones((N, 1))*x.mean(0)\n",
    "X_standard = X_standard*(1/np.std(X_standard,0))\n",
    "\n",
    "X = X_standard\n",
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0060303346\n"
     ]
    }
   ],
   "source": [
    "# import toolbox_02450\n",
    "from matplotlib.pyplot import figure, plot, xlabel, ylabel, legend, show\n",
    "import sklearn.linear_model as lm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Fit ordinary least squares regression model\n",
    "# model = lm.LinearRegression(fit_intercept=True)\n",
    "model = lm.Ridge(alpha = 1.0, fit_intercept=True)\n",
    "model = model.fit(X,y)\n",
    "# Compute model output:\n",
    "y_est = model.predict(X)\n",
    "loss = nn.MSELoss()(torch.from_numpy(y).to(device), torch.from_numpy(y_est).to(device))\n",
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006003649539698801\n"
     ]
    }
   ],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, weight = 0.0) :\n",
    "        self.weight = weight\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self,  X: np.array, y: np.array):\n",
    "        x0 = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,x0),axis=1)\n",
    "        X = np.matrix(X)\n",
    "        y = np.matrix(y)\n",
    "        w = np.matmul(X.T, X)\n",
    "        w = w + self.weight * np.eye(X.shape[1], X.shape[1])\n",
    "        w = w.I * (X.T * y)\n",
    "        self.w = w\n",
    "        return w\n",
    "    \n",
    "    def predict(self, X: np.array):\n",
    "        x0 = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,x0),axis=1)\n",
    "        X = np.matrix(X)\n",
    "        y_pred = X * self.w\n",
    "        return y_pred\n",
    "\n",
    "model = Linear_Regression(0.0)\n",
    "w = model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "loss = nn.MSELoss()(torch.from_numpy(y).to(device), torch.from_numpy(y_pred).to(device))\n",
    "print(loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060845792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([13611])) that is different to the input size (torch.Size([13611, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "model = DummyRegressor(strategy=\"mean\")\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "loss = nn.MSELoss()(torch.from_numpy(y).to(device), torch.from_numpy(y_pred).to(device))\n",
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.pylab import figure, plot, xlabel, ylabel, legend, ylim, show\n",
    "import sklearn.linear_model as lm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, num_input, num_output, num_hidden):\n",
    "        super(Neural_Net, self).__init__()\n",
    "        self.Net = nn.Sequential(\n",
    "            nn.Linear(num_input, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_output)\n",
    "        )\n",
    "        torch.nn.init.xavier_uniform_(self.Net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.Net[2].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.Net[4].weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.Net(input)\n",
    "\n",
    "class ANN():\n",
    "    def __init__(self, num_input, num_output, num_hidden, max_iters):\n",
    "        self.num_input = num_input\n",
    "        self.num_output = num_output\n",
    "        self.num_hidden = num_hidden\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = Neural_Net(num_input, num_output, num_hidden).to(self.device)\n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        self.max_iters = max_iters\n",
    "        self.path_root = 'ANN_model/'\n",
    "    \n",
    "    def train(self, X: np.array, y: np.array):\n",
    "        x_train_ = torch.from_numpy(X).to(self.device)\n",
    "        y_train_ = torch.from_numpy(y).to(self.device)\n",
    "        self.model.train()\n",
    "        optim = torch.optim.Adam(self.model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optim, 3e4, gamma=0.1, last_epoch=-1)\n",
    "        for i in range(self.max_iters):\n",
    "            output = self.model(x_train_)\n",
    "            loss = self.loss_func(output, y_train_)\n",
    "            if loss.cpu().detach().numpy() < 1e-6:\n",
    "                print(\"early stop\")\n",
    "                break\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # if i % 10000 == 0:\n",
    "            #     print('steps: {}, loss: {}'.format(i, loss.cpu().detach().numpy()))\n",
    "            scheduler.step()\n",
    "\n",
    "    def predict(self, X: np.array):\n",
    "        self.model.eval()\n",
    "        x_test = torch.from_numpy(X).to(self.device)\n",
    "        output = self.model(x_test).cpu().detach().numpy()\n",
    "        return output\n",
    "    \n",
    "    def save_model(self, k1, k2):\n",
    "        PATH = self.path_root + \"ANN_{}_{}_{}.pt\".format(k1, k2, self.num_hidden)\n",
    "        torch.save(self.model.state_dict(), PATH)\n",
    "    \n",
    "    def load_model(self, k1, k2):\n",
    "        PATH = self.path_root + \"ANN_{}_{}_{}.pt\".format(k1, k2, self.num_hidden)\n",
    "        self.model = Neural_Net(self.num_input, self.num_output, self.num_hidden).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(PATH))\n",
    "        \n",
    "# print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ANN(X.shape[1], 1, 64, 10000)\n",
    "# model.load_model(0,0)\n",
    "# model.train(X, y)\n",
    "# model.save_model(0, 0)\n",
    "# output = model.predict(X)\n",
    "# error = torch.nn.MSELoss()(torch.from_numpy(output).to(device), torch.from_numpy(y.reshape((-1))).to(device))\n",
    "# error = error.cpu().detach().numpy()\n",
    "# print(\"ANN Model error: {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "params = {}\n",
    "params['Linear_Regression'] = {'lambda': [1.0, 1e-1, 1e-2, 1e-3]}\n",
    "params['Baseline'] = [None]\n",
    "params['ANN'] = {'hidden_layer_sizes': [1, 16, 64]}\n",
    "\n",
    "def twolevelcv_reg(X: np.array, y: np.array, k1: int, k2: int, rs: int, params: dict):\n",
    "    loss_func = nn.MSELoss()\n",
    "    k_1 = 0\n",
    "    names = [n for n in params.keys()]\n",
    "    col_names = ['Param. Value', 'Error']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = KFold(n_splits=k1)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k_1 += 1\n",
    "        kf2 = KFold(n_splits=k2)\n",
    "        print(f'Computing KFold {k_1}/{k1}...')\n",
    "        # second level split\n",
    "        k_2 = 0\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            k_2 += 1\n",
    "            X_train = X[train_idx2, :]\n",
    "            y_train = y[train_idx2]\n",
    "            X_test = X[test_idx2, :]\n",
    "            y_test = y[test_idx2]\n",
    "            generalization_error_test_x = X[test_idx1, :]\n",
    "            generalization_error_test_y = y[test_idx1]\n",
    "            for name in names:\n",
    "                if name == 'Linear_Regression':\n",
    "                    error_test = []\n",
    "                    for i in params[name]['lambda']:\n",
    "                        model = lm.Ridge(alpha = i, fit_intercept=True)\n",
    "                        model = model.fit(X_train, y_train)\n",
    "                        # Compute model output:\n",
    "                        y_est = model.predict(X_test)\n",
    "                        error = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                                            torch.from_numpy(y_est.reshape((-1))).to(device))\n",
    "                        error = error.cpu().detach().numpy()\n",
    "                        print(\"LR: {}, {}\".format(i, error))\n",
    "                        error_test.append(error)\n",
    "                    min_param = params[name]['lambda'][np.argmin(error_test)]\n",
    "                elif name == 'ANN':\n",
    "                    error_test = []\n",
    "                    for i in params[name]['hidden_layer_sizes']:\n",
    "                        ANN_model = ANN(X_train.shape[1], 1, i, int(1e5))\n",
    "                        ANN_model.load_model(k_1, k_2)\n",
    "                        ANN_model.train(X_train, y_train)\n",
    "                        ANN_model.save_model(k_1, k_2)\n",
    "                        output = ANN_model.predict(X_test)\n",
    "                        error = loss_func(torch.from_numpy(output).to(device), torch.from_numpy(y_test.reshape((-1))).to(device))\n",
    "                        error = error.cpu().detach().numpy()\n",
    "                        print(\"NN: {}, {}\".format(i, error))\n",
    "                        error_test.append(error)\n",
    "                        print(\"ANN Model error: {}\".format(error))\n",
    "                    min_param = params[name]['hidden_layer_sizes'][np.argmin(error_test)]\n",
    "                else:\n",
    "                    model = DummyRegressor(strategy=\"mean\")\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    error = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                        torch.from_numpy(y_pred.reshape((-1))).to(device))\n",
    "                    error = error.cpu().detach().numpy()\n",
    "                    print(\"BL: {}\".format(error))\n",
    "                    error_test = error\n",
    "                    min_param = np.NaN\n",
    "                df.loc(axis = 1)[name, 'Error'][k_1] = np.min(error_test)\n",
    "                df.loc(axis = 1)[name, 'Param. Value'][k_1] = min_param\n",
    "                \n",
    "    return df, test_idx1\n",
    "\n",
    "# Table, test_set_outer = twolevelcv_reg(X, y, 5, 5, 1, params)\n",
    "# Table.to_csv('Results/Regression.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 Generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "params = {}\n",
    "params['Linear_Regression'] = {'lambda': [1.0, 1e-1, 1e-2, 1e-3]}\n",
    "params['Baseline'] = [None]\n",
    "params['ANN'] = {'hidden_layer_sizes': [1, 16, 64]}\n",
    "\n",
    "def twolevelcv_reg_generalization_error(X: np.array, y: np.array, k1: int, k2: int, rs: int, params: dict):\n",
    "    loss_func = nn.MSELoss()\n",
    "    k_1 = 0\n",
    "    names = [n for n in params.keys()]\n",
    "    col_names = ['Param. Value', 'Error', 'Generalization Error']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = KFold(n_splits=k1)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k_1 += 1\n",
    "        kf2 = KFold(n_splits=k2)\n",
    "        print(f'Computing KFold {k_1}/{k1}...')\n",
    "        # second level split\n",
    "        k_2 = 0\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            k_2 += 1\n",
    "            X_train = X[train_idx2, :]\n",
    "            y_train = y[train_idx2]\n",
    "            X_test = X[test_idx2, :]\n",
    "            y_test = y[test_idx2]\n",
    "            generalization_error_test_x = X[test_idx1, :]\n",
    "            generalization_error_test_y = y[test_idx1]\n",
    "            for name in names:\n",
    "                if name == 'Linear_Regression':\n",
    "                    error_test = []\n",
    "                    gen_error_test = []\n",
    "                    for i in params[name]['lambda']:\n",
    "                        model = lm.Ridge(alpha = i, fit_intercept=True)\n",
    "                        model = model.fit(X_train, y_train)\n",
    "                        joblib.dump(model, \"Linear_Regression/LR_{}_{}_{}.pkl\".format(k_1, k_2, i))\n",
    "                        # Compute model output:\n",
    "                        y_est = model.predict(X_test)\n",
    "                        error = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                                            torch.from_numpy(y_est.reshape((-1))).to(device))\n",
    "                        error = error.cpu().detach().numpy()\n",
    "                        error_test.append(error)\n",
    "                        print(\"LR: {}, {}\".format(i, error))\n",
    "                        gen_test_y = model.predict(generalization_error_test_x)\n",
    "                        gen_error = nn.MSELoss()(torch.from_numpy(generalization_error_test_y.reshape((-1))).to(device), \\\n",
    "                                            torch.from_numpy(gen_test_y.reshape((-1))).to(device))\n",
    "                        gen_error = gen_error.cpu().detach().numpy()\n",
    "                        gen_error_test.append(gen_error)\n",
    "                    min_param = params[name]['lambda'][np.argmin(error_test)]\n",
    "                    gen_error_out = gen_error_test[np.argmin(error_test)]\n",
    "                    error_out = error_test[np.argmin(error_test)]\n",
    "                elif name == 'ANN':\n",
    "                    error_test = []\n",
    "                    gen_error_test = []\n",
    "                    for i in params[name]['hidden_layer_sizes']:\n",
    "                        ANN_model = ANN(X_train.shape[1], 1, i, int(1e5))\n",
    "                        ANN_model.load_model(k_1, k_2)\n",
    "                        ANN_model.train(X_train, y_train)\n",
    "                        ANN_model.save_model(k_1, k_2)\n",
    "                        output = ANN_model.predict(X_test)\n",
    "                        error = loss_func(torch.from_numpy(output).to(device), torch.from_numpy(y_test.reshape((-1))).to(device))\n",
    "                        error = error.cpu().detach().numpy()\n",
    "                        print(\"NN: {}, {}\".format(i, error))\n",
    "                        error_test.append(error)\n",
    "                        gen_test_y = model.predict(generalization_error_test_x)\n",
    "                        gen_error = nn.MSELoss()(torch.from_numpy(generalization_error_test_y.reshape((-1))).to(device), \\\n",
    "                                            torch.from_numpy(gen_test_y.reshape((-1))).to(device))\n",
    "                        gen_error = gen_error.cpu().detach().numpy()\n",
    "                        gen_error_test.append(gen_error)\n",
    "                    min_param = params[name]['hidden_layer_sizes'][np.argmin(error_test)]\n",
    "                    gen_error_out = gen_error_test[np.argmin(error_test)]\n",
    "                    error_out = error_test[np.argmin(error_test)]\n",
    "                else:\n",
    "                    model = DummyRegressor(strategy=\"mean\")\n",
    "                    model.fit(X_train, y_train)\n",
    "                    joblib.dump(model, \"BaseLine/BL_{}_{}.pkl\".format(k_1, k_2))\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    error = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                        torch.from_numpy(y_pred.reshape((-1))).to(device))\n",
    "                    error = error.cpu().detach().numpy()\n",
    "                    print(\"NN: {}\".format(error))\n",
    "                    error_test = error\n",
    "                    gen_test_y = model.predict(generalization_error_test_x)\n",
    "                    gen_error = nn.MSELoss()(torch.from_numpy(generalization_error_test_y.reshape((-1))).to(device), \\\n",
    "                                            torch.from_numpy(gen_test_y.reshape((-1))).to(device))\n",
    "                    gen_error = gen_error.cpu().detach().numpy()\n",
    "                    min_param = np.NaN\n",
    "                    gen_error_out = gen_error_test[np.argmin(error_test)]\n",
    "                    error_out = error\n",
    "                df.loc(axis = 1)[name, 'Error'][k_1] = error_out\n",
    "                df.loc(axis = 1)[name, 'Param. Value'][k_1] = min_param\n",
    "                df.loc(axis = 1)[name, 'Generalization Error'][k_1] = gen_error_out\n",
    "                \n",
    "    return df, test_idx1\n",
    "\n",
    "# Table, test_set_outer = twolevelcv_reg_generalization_error(X, y, 5, 5, 1, params)\n",
    "# Table.to_csv('Results/Regression_Generalization_Error.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sectin 3 Compare of different models using Setup 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P-value and Confidencial Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "mA = joblib.load('Linear_Regression/LR_1.pkl')\n",
    "mB = joblib.load('BaseLine/BL_1.pkl')\n",
    "def P_value_CI(mA, mB, X, y):\n",
    "    yhatA = mA.predict(X)\n",
    "    yhatA = np.reshape(yhatA, (-1,1))\n",
    "    yhatB = mB.predict(X)\n",
    "    yhatB = np.reshape(yhatB, (-1,1))\n",
    "    zA = np.abs(y - yhatA ) ** 2\n",
    "\n",
    "    # compute confidence interval of model A\n",
    "    alpha = 0.05\n",
    "    CIA = st.t.interval(1-alpha, df=len(zA)-1, loc=np.mean(zA), scale=st.sem(zA))  # Confidence interval\n",
    "\n",
    "    # Compute confidence interval of z = zA-zB and p-value of Null hypothesis\n",
    "    zB = np.abs(y - yhatB ) ** 2\n",
    "    z = zA - zB\n",
    "    CI = st.t.interval(1-alpha, len(z)-1, loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "    p = 2*st.t.cdf( -np.abs( np.mean(z) )/st.sem(z), df=len(z)-1)  # p-value\n",
    "    s, _ = st.ttest_rel(zA, zB)\n",
    "    return p[0], [CI[0][0], CI[1][0]], s[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paired T test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models_comparison(X: np.array, y: np.array, k1: int, k2: int, rs: int, params: dict):\n",
    "    loss_func = nn.MSELoss()\n",
    "    k_1 = 0\n",
    "    names = ['LR vs NN', 'LR vs BL', 'NN vs BL']\n",
    "    col_names = ['t-test', 'p-value', 'Confidenctial Interval']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = KFold(n_splits=k1)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k_1 += 1\n",
    "        print(f'Computing KFold {k_1}/{k1}...')\n",
    "        # second level split\n",
    "        X_train = X[train_idx1, :]\n",
    "        y_train = y[train_idx1]\n",
    "        X_test = X[test_idx1, :]\n",
    "        y_test = y[test_idx1]\n",
    "        model_LR = lm.Ridge(alpha = 0.001, fit_intercept=True)\n",
    "        model_LR = model_LR.fit(X_train, y_train)\n",
    "        joblib.dump(model_LR, \"Linear_Regression/LR_{}.pkl\".format(k_1))\n",
    "        y_est_lr = model_LR.predict(X_test)\n",
    "        error_lr = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                            torch.from_numpy(y_est_lr.reshape((-1))).to(device)).cpu().detach().numpy()\n",
    "        print(error_lr)\n",
    "        model_NN = ANN(X_train.shape[1], 1, 64, int(1e5))\n",
    "        model_NN.load_model(k_1, 0)\n",
    "        y_est_nn = model_NN.predict(X_test)\n",
    "        error_nn = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                            torch.from_numpy(y_est_nn.reshape((-1))).to(device)).cpu().detach().numpy()\n",
    "        print(error_nn)\n",
    "        model_BL = DummyRegressor(strategy=\"mean\")\n",
    "        model_BL = model_BL.fit(X_train, y_train)\n",
    "        y_est_bl = model_BL.predict(X_test)\n",
    "        error_bl = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                            torch.from_numpy(y_est_bl.reshape((-1))).to(device)).cpu().detach().numpy()\n",
    "        print(error_bl)\n",
    "        joblib.dump(model_BL, \"BaseLine/BL_{}.pkl\".format(k_1))\n",
    "        \n",
    "        for name in names:\n",
    "            if name == 'LR vs NN':\n",
    "                p, CI, stats = P_value_CI(model_LR, model_NN, X_test, y_test)\n",
    "                df.loc(axis = 1)[name, 't-test'][k_1] = stats\n",
    "                df.loc(axis = 1)[name, 'p-value'][k_1] = p\n",
    "                df.loc(axis = 1)[name, 'Confidenctial Interval'][k_1] = CI\n",
    "            elif 'LR vs BL':\n",
    "                p, CI, stats = P_value_CI(model_LR, model_BL, X_test, y_test)\n",
    "                df.loc(axis = 1)[name, 't-test'][k_1] = stats\n",
    "                df.loc(axis = 1)[name, 'p-value'][k_1] = p\n",
    "                df.loc(axis = 1)[name, 'Confidenctial Interval'][k_1] = CI\n",
    "            else:\n",
    "                p, CI, stats = P_value_CI(model_NN, model_BL, X_test, y_test)\n",
    "                df.loc(axis = 1)[name, 't-test'][k_1] = stats\n",
    "                df.loc(axis = 1)[name, 'p-value'][k_1] = p\n",
    "                df.loc(axis = 1)[name, 'Confidenctial Interval'][k_1] = CI\n",
    "        \n",
    "    return df, test_idx1\n",
    "\n",
    "# Table, test_set_outer = Models_comparison(X, y, 5, 5, 1, params)\n",
    "# Table.to_csv('Results/Regression_P_value.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KFold 1/5...\n",
      "0.10210502\n",
      "0.0017172308\n",
      "0.13367957\n",
      "Computing KFold 2/5...\n",
      "0.04001962\n",
      "3.946434e-05\n",
      "0.028156644\n",
      "Computing KFold 3/5...\n",
      "0.04001962\n",
      "3.946434e-05\n",
      "0.028156644\n",
      "Computing KFold 4/5...\n",
      "0.002262841\n",
      "1.1131615e-06\n",
      "0.013565638\n",
      "Computing KFold 5/5...\n",
      "0.00165757\n",
      "5.0719814e-07\n",
      "0.013565638\n"
     ]
    }
   ],
   "source": [
    "def Best_Models_comparison(X: np.array, y: np.array, k1: int, k2: int, rs: int, params: dict):\n",
    "    loss_func = nn.MSELoss()\n",
    "    k_1 = 0\n",
    "    names = ['LR vs NN', 'LR vs BL', 'NN vs BL']\n",
    "    col_names = ['t-test', 'p-value', 'Confidenctial Interval']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = KFold(n_splits=k1)\n",
    "    # first level split\n",
    "    error_LR = 1e5\n",
    "    error_NN = 1e5\n",
    "    error_BL = 1e5\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k_1 += 1\n",
    "        print(f'Computing KFold {k_1}/{k1}...')\n",
    "        # second level split\n",
    "        X_train = X[train_idx1, :]\n",
    "        y_train = y[train_idx1]\n",
    "        X_test = X[test_idx1, :]\n",
    "        y_test = y[test_idx1]\n",
    "        model_LR = lm.Ridge(alpha = 0.001, fit_intercept=True)\n",
    "        model_LR = model_LR.fit(X_train, y_train)\n",
    "        y_est_lr = model_LR.predict(X_test)\n",
    "        error_lr = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                            torch.from_numpy(y_est_lr.reshape((-1))).to(device)).cpu().detach().numpy()\n",
    "        if error_lr < error_LR:\n",
    "            error_LR = error_lr\n",
    "            model_LR_best = model_LR\n",
    "        print(error_LR)\n",
    "        model_NN = ANN(X_train.shape[1], 1, 64, int(1e5))\n",
    "        # model_NN.train(X_train, y_train)\n",
    "        model_NN.load_model(k_1, 0)\n",
    "        y_est_nn = model_NN.predict(X_test)\n",
    "        error_nn = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                            torch.from_numpy(y_est_nn.reshape((-1))).to(device)).cpu().detach().numpy()\n",
    "        if error_nn < error_NN:\n",
    "            error_NN = error_nn\n",
    "            model_NN_best = model_NN\n",
    "        print(error_NN)\n",
    "        model_BL = DummyRegressor(strategy=\"mean\")\n",
    "        model_BL = model_BL.fit(X_train, y_train)\n",
    "        y_est_bl = model_BL.predict(X_test)\n",
    "        error_bl = nn.MSELoss()(torch.from_numpy(y_test.reshape((-1))).to(device), \\\n",
    "                            torch.from_numpy(y_est_bl.reshape((-1))).to(device)).cpu().detach().numpy()\n",
    "        if error_bl < error_BL:\n",
    "            error_BL = error_bl\n",
    "            model_BL_best = model_BL\n",
    "        print(error_BL)\n",
    "        \n",
    "        for name in names:\n",
    "            if name == 'LR vs NN':\n",
    "                p, CI, stats = P_value_CI(model_LR_best, model_NN_best, X_test, y_test)\n",
    "                df.loc(axis = 1)[name, 't-test'][k_1] = stats\n",
    "                df.loc(axis = 1)[name, 'p-value'][k_1] = p\n",
    "                df.loc(axis = 1)[name, 'Confidenctial Interval'][k_1] = CI\n",
    "            elif 'LR vs BL':\n",
    "                p, CI, stats = P_value_CI(model_LR_best, model_BL_best, X_test, y_test)\n",
    "                df.loc(axis = 1)[name, 't-test'][k_1] = stats\n",
    "                df.loc(axis = 1)[name, 'p-value'][k_1] = p\n",
    "                df.loc(axis = 1)[name, 'Confidenctial Interval'][k_1] = CI\n",
    "            else:\n",
    "                p, CI, stats = P_value_CI(model_NN_best, model_BL_best, X_test, y_test)\n",
    "                df.loc(axis = 1)[name, 't-test'][k_1] = stats\n",
    "                df.loc(axis = 1)[name, 'p-value'][k_1] = p\n",
    "                df.loc(axis = 1)[name, 'Confidenctial Interval'][k_1] = CI\n",
    "        \n",
    "    return df, test_idx1, model_LR_best\n",
    "\n",
    "Table, test_set_outer, model_LR_best = Best_Models_comparison(X, y, 5, 5, 1, params)\n",
    "Table.to_csv('Results/Best_Regression_P_value.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6070746 , -0.24132958,  0.21022192,  1.6769819 ,  0.12643151,\n",
       "        -0.013324  ,  0.05160747, -0.10550004, -0.03378718]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_best.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad43814",
   "metadata": {},
   "source": [
    "# **2 - Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying object without editing the original\n",
    "df_ = df.copy(deep=True)\n",
    "# Doing this we can choose to use outliers filter or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_.columns.values\n",
    "X = df_.drop(columns='Class').values\n",
    "y = df_['Class']\n",
    "le = LabelEncoder()\n",
    "y_ = le.fit_transform(y)\n",
    "classes = y.unique()\n",
    "\n",
    "print('· NUMBER OF FEATURES:', X.shape[1])\n",
    "print('\\n· FEATURES:', columns[:-1])\n",
    "print('\\n· NUMBER OF DATA POINTS:', X.shape[0])\n",
    "print('\\n· CLASSES:', classes)\n",
    "print('\\n· NUMBER OF CLASSES:', len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold_ = 3\n",
    "outlier_index = []\n",
    "df_ = pd.DataFrame(columns=df.columns)\n",
    "index = 0\n",
    "for K in classes:\n",
    "    outlier_index = []\n",
    "    a = df.loc[df[\"Class\"] == K]\n",
    "    value = a.drop(columns='Class').values\n",
    "    for j in range(16):\n",
    "        std = np.std(value[:, j])\n",
    "        mean = np.mean(value[:, j])\n",
    "        for i in range(value[:, j].shape[0]):\n",
    "            if (value[i, j] - mean) / std > Threshold_:\n",
    "                outlier_index.append(i + index)\n",
    "    index = i + index + 1\n",
    "    outlier_index = np.unique(outlier_index)\n",
    "    a = a.drop(outlier_index)\n",
    "    df_ = pd.concat([df_,a])\n",
    "df_.reset_index(drop=True, inplace=True)\n",
    "print(f'Filtered outliers: {df.shape[0] - df_.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarization of the dataset\n",
    "sc = StandardScaler()\n",
    "X_stdz = sc.fit_transform(X)\n",
    "df_stdz = pd.DataFrame(columns = columns[:-1], data = X_stdz)\n",
    "df_stdz['Class'] = y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Logistic regression *vs.* Neural Network *vs.* Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # evaluate the model and collect the scores\n",
    "# n_scores = cross_val_score(model, X_stdz, y_, scoring = 'accuracy', cv=cv, n_jobs=-1)\n",
    "# # report the model performance\n",
    "# print('Mean Accuracy: %.3f (+-%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Cross-Validation table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "lam = np.logspace(-6, 2, 100)\n",
    "C = 1/ lam\n",
    "# C = [200000000, 10000000, 0.1519911082952933, 0.2848035868435805 ]\n",
    "params['LogisticRegression'] = {'C': C}\n",
    "params['DummyClassifier'] = [None]\n",
    "params['MLPClassifier'] = {'hidden_layer_sizes': [(8, ), (16, ), (20, )]}\n",
    "models = [LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000000, random_state= random_state, tol = 0.003, n_jobs = -1),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=random_state),\n",
    "        MLPClassifier(solver='adam', activation='logistic', alpha=1e-4, random_state=random_state, max_iter=1000, \n",
    "        early_stopping=True, validation_fraction=0.2, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.01)]\n",
    "k1 = 10\n",
    "k2 = 10\n",
    "Table, test_set_outer = twolevelcv(X = X_stdz, y = y_, k1 = k1, k2 = k2, models = models, params = params, rs = random_state)\n",
    "Table.to_csv('Results/Test2_saga.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table = Table.round(3)\n",
    "Table.to_csv(r'Results\\Table_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_outer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Stadistical Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def McNemar(models: list, X: np.array, y: np.array, k1: int, rs: int):\n",
    "    \"\"\"Computes the McNemar matrix for all the models in the list \n",
    "\n",
    "    Args:\n",
    "        models (list): list of models\n",
    "        X (np.array): Features\n",
    "        y (np.array): Classes\n",
    "        k1 (int): Number of folds\n",
    "        rs (int): Random state\n",
    "\n",
    "    Returns:\n",
    "        _type_: Matrix as a dictionary\n",
    "    \"\"\"\n",
    "    kf1 = StratifiedKFold(k1, shuffle = True, random_state=rs)\n",
    "    k = 0\n",
    "    # setting up all the possible combinations between the different models\n",
    "    matrix = dict.fromkeys(combinations(range(len(models)), 2))\n",
    "    matrix_tot = dict.fromkeys(combinations(range(len(models)), 2))\n",
    "    for train_idx, test_idx in kf1.split(X, y):\n",
    "        test_size = test_idx.shape[0]\n",
    "        yABC = np.empty(shape=(len(models), test_size))\n",
    "        for i, model in enumerate(models):\n",
    "            model.fit(X[train_idx,:], y[train_idx])\n",
    "            y_pred = model.predict(X[test_idx, :])\n",
    "            yABC[i, :] = 1*(y_pred == y[test_idx])\n",
    "        for j in list(matrix.keys()):\n",
    "            if k == 0:\n",
    "                matrix[j] = np.empty(shape=(k1, 4))\n",
    "            n11 = np.sum(yABC[j[0],:]*yABC[j[1],:])\n",
    "            n12 = np.sum(yABC[j[0],:]*(1-yABC[j[1],:]))\n",
    "            n21 = np.sum(yABC[j[1],:]*(1-yABC[j[0],:]))\n",
    "            n22 = np.sum((1-yABC[0,:])*(1-yABC[1,:]))\n",
    "            matrix[j][k] = np.array([n11, n12, n21, n22])\n",
    "            matrix_tot[j] = matrix[j].sum(axis = 0)\n",
    "        k+=1\n",
    "    return matrix, matrix_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "models = [LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state= random_state, C = 0.07),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=random_state),\n",
    "        MLPClassifier(solver='adam', activation='logistic', alpha=1e-4, random_state=random_state, max_iter=1000, hidden_layer_sizes=(8, ),\n",
    "        early_stopping=True, validation_fraction=0.2, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.01)]\n",
    "k1 = 10\n",
    "mfull, m_tot = McNemar(models, X_stdz[test_set_outer, :], y_[test_set_outer], k1, rs = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mat in mfull:\n",
    "    print('          ',mat)\n",
    "    for ma in mfull[mat]:\n",
    "        print('      ', ma)\n",
    "    print('\\nTotal:',m_tot[mat])\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the matrix with shape (3: number of models, 10: nº of k-folds, 4: matrix shape)\n",
    "\n",
    "The matrix is squeezed so we have:\n",
    "$$\\begin{bmatrix}\n",
    " n_{11}    & n_{12}  \\\\\n",
    " n_{21}    & n_{22}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here is $[ n_{11}, n_{12}, n_{21}, n_{22} ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0:$ Model A has the same performarnce as model B \n",
    "\n",
    "$H_1:$ Model A and model B has different performance\n",
    "\n",
    "small p_value-> we discard H0 -> Model A and Model B have different performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we set:\n",
    "\n",
    "**Model 0**: LR\n",
    "\n",
    "**Model 1**: Baseline\n",
    "\n",
    "**Model 2**: MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***P-Values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = list(combinations(range(len(models)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "pv_dict = dict.fromkeys(combs)\n",
    "for j in combs:\n",
    "    vals = m[j][:, 1:3]\n",
    "    pv_dict[j] = [binom.cdf(min(vals[i]), n = sum(vals[i]), p = 1/2) for i in range(len(vals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McNemar_pv = pd.DataFrame(columns = combs, index = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for n in pv_dict.values():\n",
    "    print(f'{combs[i]}')\n",
    "    for j, k in enumerate(n):\n",
    "        McNemar_pv[combs[i]][j] = \"{:.3e}\".format(k)\n",
    "        text = str(\"${:.2e}\".format(k))\n",
    "        text = text + '}$'\n",
    "        text = text.replace(\"e\", \"·10^{\")\n",
    "        print(text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McNemar_pv.to_csv('Results/McNemar_pv_best.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcs(mat: np.array):\n",
    "    \"\"\"Calculate f y g from a McNemar Matrix\n",
    "    Args:\n",
    "        matrix (np.array): McNemar matrix from one K-fold\n",
    "    Returns:\n",
    "        _type_: f and g\n",
    "    \"\"\"\n",
    "    n = mat.sum()\n",
    "    n12 = mat[1]\n",
    "    n21 = mat[2]\n",
    "    E_th = (n12 - n21)/n \n",
    "    Q = (n**2 * (n+1) * (E_th +1) * (1-E_th)) / (n * (n12 + n21) - (n12 - n21)**2)\n",
    "    f = (Q-1)*(E_th+1)/2\n",
    "    g = (Q-1)*(1-E_th)/2\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval(mat: np.array, alpha: float):\n",
    "    \"\"\"McNemar confidence interval\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The desired confidence (should be 0.05)  \n",
    "        f (_type_): output of calcs\n",
    "        g (_type_): output of calcs \n",
    "\n",
    "    Returns:\n",
    "        _type_: left and right bounds from the interval\n",
    "    \"\"\"\n",
    "    f,g = calcs(mat)\n",
    "    theta_L = 2*beta.ppf(alpha, f, g) - 1\n",
    "    theta_R = 2*beta.ppf(1 - alpha/2, f, g) - 1\n",
    "    return theta_L, theta_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in m:\n",
    "    print(i)\n",
    "    for mat in m[i]:\n",
    "        theta_L, theta_R = interval(mat, 0.05)\n",
    "        print(f'[{np.round(theta_L, 2)}, {np.round(theta_R, 2)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Train logistic regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the fourth exercise do we have to repeat the parameter selection process or can just go ahead with the best parameter selection for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3051d3c5513a3f8d66cffc6293c1d26977f11d89a064d2eaf959b056923e92ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
